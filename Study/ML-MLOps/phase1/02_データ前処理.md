# Phase 1-2: データ前処理

## 学習目標

この単元を終えると、以下ができるようになります：

- 欠損値の処理ができる
- 特徴量エンジニアリングができる
- データの標準化・正規化ができる

## データ前処理の重要性

```mermaid
graph LR
    R[生データ] --> C[クリーニング]
    C --> T[変換]
    T --> E[特徴量エンジニアリング]
    E --> M[モデル学習]
```

**「ゴミを入れればゴミが出る」** - データの品質がモデルの品質を決める

## ハンズオン

### 演習1: 欠損値の処理

```python
# missing_values.py
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer

# サンプルデータ
df = pd.DataFrame({
    'age': [25, 30, np.nan, 35, 40],
    'salary': [50000, np.nan, 60000, np.nan, 80000],
    'department': ['Sales', 'IT', 'IT', np.nan, 'Sales']
})

print('元データ:')
print(df)
print(f'\n欠損値:\n{df.isnull().sum()}')

# 方法1: 削除
df_dropped = df.dropna()
print(f'\n削除後: {len(df_dropped)} 行')

# 方法2: 数値は平均で補完
num_imputer = SimpleImputer(strategy='mean')
df['age_filled'] = num_imputer.fit_transform(df[['age']])
df['salary_filled'] = num_imputer.fit_transform(df[['salary']])

# 方法3: カテゴリは最頻値で補完
cat_imputer = SimpleImputer(strategy='most_frequent')
df['department_filled'] = cat_imputer.fit_transform(df[['department']]).ravel()

print('\n補完後:')
print(df)
```

### 演習2: カテゴリ変数のエンコーディング

```python
# encoding.py
import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

df = pd.DataFrame({
    'color': ['red', 'blue', 'green', 'red', 'blue'],
    'size': ['S', 'M', 'L', 'M', 'S']
})

print('元データ:')
print(df)

# Label Encoding（順序あり）
le = LabelEncoder()
df['size_encoded'] = le.fit_transform(df['size'])
print(f'\nLabel Encoding: {dict(zip(le.classes_, range(len(le.classes_))))}')

# One-Hot Encoding（順序なし）
df_onehot = pd.get_dummies(df['color'], prefix='color')
print('\nOne-Hot Encoding:')
print(df_onehot)

# 結合
result = pd.concat([df, df_onehot], axis=1)
print('\n最終結果:')
print(result)
```

### 演習3: スケーリング

```python
# scaling.py
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# サンプルデータ
X = np.array([
    [25, 50000],    # age, salary
    [30, 60000],
    [35, 80000],
    [40, 100000]
])

print('元データ:')
print(X)

# StandardScaler: 平均0, 標準偏差1
scaler_std = StandardScaler()
X_std = scaler_std.fit_transform(X)
print('\nStandardScaler:')
print(X_std)
print(f'平均: {X_std.mean(axis=0)}, 標準偏差: {X_std.std(axis=0)}')

# MinMaxScaler: 0-1の範囲
scaler_mm = MinMaxScaler()
X_mm = scaler_mm.fit_transform(X)
print('\nMinMaxScaler:')
print(X_mm)
print(f'最小: {X_mm.min(axis=0)}, 最大: {X_mm.max(axis=0)}')
```

### 演習4: 特徴量エンジニアリング

```python
# feature_engineering.py
import pandas as pd
import numpy as np

# ECサイトのデータ
df = pd.DataFrame({
    'user_id': [1, 1, 2, 2, 2, 3],
    'purchase_date': pd.to_datetime([
        '2024-01-01', '2024-01-15',
        '2024-01-05', '2024-01-20', '2024-02-01',
        '2024-01-10'
    ]),
    'amount': [1000, 2000, 500, 1500, 3000, 800]
})

print('元データ:')
print(df)

# 日付から特徴量を抽出
df['day_of_week'] = df['purchase_date'].dt.dayofweek
df['month'] = df['purchase_date'].dt.month
df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)

# 集約特徴量
user_features = df.groupby('user_id').agg({
    'amount': ['count', 'sum', 'mean', 'max'],
    'purchase_date': ['min', 'max']
}).reset_index()

user_features.columns = [
    'user_id', 
    'purchase_count', 'total_amount', 'avg_amount', 'max_amount',
    'first_purchase', 'last_purchase'
]

# RFM分析
reference_date = pd.to_datetime('2024-02-15')
user_features['recency'] = (reference_date - user_features['last_purchase']).dt.days
user_features['frequency'] = user_features['purchase_count']
user_features['monetary'] = user_features['total_amount']

print('\nユーザー特徴量（RFM）:')
print(user_features[['user_id', 'recency', 'frequency', 'monetary']])
```

### 演習5: パイプライン

```python
# pipeline.py
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.tree import DecisionTreeClassifier
import pandas as pd
import numpy as np

# サンプルデータ
df = pd.DataFrame({
    'age': [25, 30, np.nan, 35, 40],
    'salary': [50000, 60000, 70000, np.nan, 90000],
    'department': ['Sales', 'IT', 'IT', 'HR', 'Sales'],
    'purchased': [0, 1, 1, 0, 1]
})

X = df.drop('purchased', axis=1)
y = df['purchased']

# 数値特徴量の前処理
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# カテゴリ特徴量の前処理
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# 列ごとに適用
preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, ['age', 'salary']),
    ('cat', categorical_transformer, ['department'])
])

# モデルと組み合わせ
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', DecisionTreeClassifier())
])

# 学習（前処理も自動で行われる）
pipeline.fit(X, y)

# 予測
predictions = pipeline.predict(X)
print(f'予測: {predictions}')
```

## 理解度確認

### 問題

特徴量のスケールが異なる場合（例: 年齢 0-100、年収 0-10000000）に適用すべき前処理はどれか。

**A.** Label Encoding

**B.** One-Hot Encoding

**C.** StandardScaler または MinMaxScaler

**D.** SimpleImputer

---

### 解答・解説

**正解: C**

特徴量のスケールが大きく異なる場合、多くのアルゴリズム（SVM、勾配降下法ベースなど）で学習に悪影響があります。StandardScaler か MinMaxScaler でスケールを揃えます。

---

## 次のステップ

データ前処理を学びました。次は回帰分析を学びましょう。

**次の単元**: [Phase 2-1: 回帰分析](../phase2/01_回帰分析.md)
