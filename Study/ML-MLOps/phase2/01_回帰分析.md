# Phase 2-1: 回帰分析

## 学習目標

この単元を終えると、以下ができるようになります：

- 線形回帰を実装できる
- 回帰の評価指標を理解できる
- 正則化の効果を理解できる

## 回帰とは

```mermaid
graph LR
    X[特徴量 X] --> M[モデル]
    M --> Y[連続値 Y]
```

**予測対象が連続値**（価格、気温、売上など）

## 線形回帰

$$y = w_1x_1 + w_2x_2 + ... + w_nx_n + b$$

| 記号 | 意味 |
|------|------|
| $y$ | 予測値 |
| $x_i$ | 特徴量 |
| $w_i$ | 重み（係数） |
| $b$ | バイアス（切片） |

## 評価指標

| 指標 | 式 | 特徴 |
|------|----|----|
| **MSE** | $\frac{1}{n}\sum(y - \hat{y})^2$ | 大きな誤差にペナルティ |
| **RMSE** | $\sqrt{MSE}$ | 元のスケール |
| **MAE** | $\frac{1}{n}\sum\|y - \hat{y}\|$ | 外れ値に強い |
| **R²** | $1 - \frac{SS_{res}}{SS_{tot}}$ | 0-1（1が理想） |

## ハンズオン

### 演習1: 線形回帰の実装

```python
# linear_regression.py
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# データ生成
np.random.seed(42)
X = np.random.rand(100, 1) * 10  # 0-10
y = 2.5 * X.flatten() + 5 + np.random.randn(100) * 2  # y = 2.5x + 5 + ノイズ

# 分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 学習
model = LinearRegression()
model.fit(X_train, y_train)

print(f'係数 (w): {model.coef_[0]:.4f}')
print(f'切片 (b): {model.intercept_:.4f}')

# 予測
y_pred = model.predict(X_test)

# 評価
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f'\nRMSE: {rmse:.4f}')
print(f'R² Score: {r2:.4f}')

# 可視化
plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.scatter(X_train, y_train, alpha=0.5, label='Train')
plt.scatter(X_test, y_test, alpha=0.5, label='Test')
plt.plot(X, model.predict(X), 'r-', label='Model')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.title('Linear Regression')

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted')

plt.tight_layout()
plt.savefig('linear_regression.png')
```

### 演習2: 多項式回帰

```python
# polynomial_regression.py
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error

# 非線形データ
np.random.seed(42)
X = np.linspace(-3, 3, 100).reshape(-1, 1)
y = 0.5 * X.flatten()**2 + X.flatten() + np.random.randn(100) * 0.5

# 各次数でモデル作成
degrees = [1, 2, 5, 15]
plt.figure(figsize=(12, 8))

for i, degree in enumerate(degrees, 1):
    model = Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('linear', LinearRegression())
    ])
    
    model.fit(X, y)
    y_pred = model.predict(X)
    mse = mean_squared_error(y, y_pred)
    
    plt.subplot(2, 2, i)
    plt.scatter(X, y, alpha=0.5)
    plt.plot(X, y_pred, 'r-', linewidth=2)
    plt.title(f'Degree={degree}, MSE={mse:.4f}')

plt.tight_layout()
plt.savefig('polynomial_regression.png')
```

### 演習3: 正則化（Ridge, Lasso）

```python
# regularization.py
import numpy as np
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error

# 高次元データ（多重共線性あり）
np.random.seed(42)
n_samples, n_features = 100, 50
X = np.random.randn(n_samples, n_features)
true_coef = np.zeros(n_features)
true_coef[:5] = [1, 2, 3, 4, 5]  # 重要な特徴量は5個だけ
y = X @ true_coef + np.random.randn(n_samples) * 0.5

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# スケーリング
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# モデル比較
models = {
    'LinearRegression': LinearRegression(),
    'Ridge (α=1.0)': Ridge(alpha=1.0),
    'Lasso (α=0.1)': Lasso(alpha=0.1)
}

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    
    # 係数がほぼ0になった特徴量の数
    n_zero = np.sum(np.abs(model.coef_) < 0.01)
    
    print(f'{name}:')
    print(f'  RMSE: {rmse:.4f}')
    print(f'  Zero coefficients: {n_zero}/{n_features}')
    print(f'  Top 5 coefficients: {np.round(model.coef_[:5], 2)}')
    print()
```

### 演習4: 住宅価格予測

```python
# housing_prediction.py
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from sklearn.pipeline import Pipeline
import numpy as np
import pandas as pd

# データ読み込み
housing = fetch_california_housing()
X, y = housing.data, housing.target

print('特徴量:')
for i, name in enumerate(housing.feature_names):
    print(f'  {name}: {X[:, i].mean():.2f} ± {X[:, i].std():.2f}')

# 分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# パイプライン
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', Ridge(alpha=1.0))
])

# 交差検証
cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error')
print(f'\nCV RMSE: {-cv_scores.mean():.4f} ± {cv_scores.std():.4f}')

# 最終評価
pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f'Test RMSE: {rmse:.4f}')

# 特徴量重要度
coef = pipeline.named_steps['model'].coef_
importance = pd.DataFrame({
    'feature': housing.feature_names,
    'coefficient': coef
}).sort_values('coefficient', key=abs, ascending=False)

print('\n特徴量重要度:')
print(importance)
```

## 理解度確認

### 問題

Lasso 回帰の特徴として正しいものはどれか。

**A.** 係数を0にしない（スパースにならない）

**B.** 係数を0にできる（特徴量選択効果）

**C.** 正則化を行わない

**D.** 非線形モデル

---

### 解答・解説

**正解: B**

Lasso 回帰は L1 正則化を使い、一部の係数を完全に0にできます。これにより自動的に特徴量選択が行われます。Ridge 回帰（L2正則化）は係数を小さくしますが0にはしません。

---

## 次のステップ

回帰分析を学びました。次は分類問題を学びましょう。

**次の単元**: [Phase 2-2: 分類問題](./02_分類問題.md)
